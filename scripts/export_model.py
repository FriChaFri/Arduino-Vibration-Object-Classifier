#!/usr/bin/env python3
"""
Export a trained MLP (produced by train_model.py) to a Teensy-friendly C header.

Typical use:
python3 scripts/export_model.py --modeldir models/latest --out include/model_weights.h
"""
from __future__ import annotations

import argparse
import json
from pathlib import Path
from typing import Callable, Iterable, List

import joblib
import numpy as np


def format_float(val: float) -> str:
    if not np.isfinite(val):
        val = 0.0
    return f"{val:.8f}f"


def format_array(
    values: Iterable,
    indent: str = "    ",
    per_line: int = 8,
    formatter: Callable[[object], str] = format_float,
) -> str:
    vals = list(values)
    if not vals:
        return ""
    lines: List[str] = []
    for i in range(0, len(vals), per_line):
        chunk = vals[i : i + per_line]
        lines.append(indent + ", ".join(formatter(v) for v in chunk))
    return ",\n".join(lines)


def load_artifacts(model_dir: Path) -> tuple:
    model_path = model_dir / "model.joblib"
    metadata_path = model_dir / "training_metadata.json"
    label_path = model_dir / "label_encoder.joblib"

    if not model_path.exists():
        raise SystemExit(f"{model_path} not found. Run train_model.py first.")
    if not metadata_path.exists():
        raise SystemExit(f"{metadata_path} not found. Run train_model.py first.")

    pipeline = joblib.load(model_path)
    metadata = json.loads(metadata_path.read_text())

    label_classes = metadata.get("label_classes")
    if not label_classes and label_path.exists():
        encoder = joblib.load(label_path)
        label_classes = [str(c) for c in getattr(encoder, "classes_", [])]
    if not label_classes:
        raise SystemExit("Label classes are missing in metadata and encoder.")

    feature_names = metadata.get("feature_names")
    if not feature_names:
        raise SystemExit("Feature names are missing in metadata (training_metadata.json).")

    try:
        imputer = pipeline.named_steps["imputer"]
        scaler = pipeline.named_steps["scaler"]
        clf = pipeline.named_steps["clf"]
    except KeyError:
        raise SystemExit("Expected pipeline with steps: imputer -> scaler -> clf (MLPClassifier).")

    clf_classes = [str(c) for c in getattr(clf, "classes_", [])]
    if clf_classes and clf_classes != label_classes:
        raise SystemExit(
            "Class ordering mismatch between metadata and classifier. Re-run training/export to refresh artifacts."
        )

    return pipeline, metadata, feature_names, label_classes, imputer, scaler, clf


def build_layer_params(clf) -> List[dict]:
    layers: List[dict] = []
    for idx, (weights, bias) in enumerate(zip(clf.coefs_, clf.intercepts_)):
        weight_matrix = np.asarray(weights, dtype=np.float32)
        bias_vec = np.asarray(bias, dtype=np.float32)
        in_dim, out_dim = weight_matrix.shape
        # Transpose to row-major [out][in] for straightforward C inference.
        flattened = weight_matrix.T.reshape(-1)
        layers.append(
            {
                "index": idx,
                "input_dim": int(in_dim),
                "output_dim": int(out_dim),
                "weights": flattened.tolist(),
                "bias": bias_vec.tolist(),
            }
        )
    return layers


def generate_header(
    feature_names: List[str],
    label_classes: List[str],
    imputer_stats: List[float],
    scaler_mean: List[float],
    scaler_scale: List[float],
    layers: List[dict],
    output_type: str,
    positive_class_index: int,
) -> str:
    header_lines: List[str] = []
    header_lines.append("// Generated by scripts/export_model.py - do not edit by hand.")
    header_lines.append("#pragma once")
    header_lines.append("#include <cstddef>")
    header_lines.append("")
    header_lines.append("namespace model_weights {")
    header_lines.append(f"constexpr std::size_t kInputDim = {len(feature_names)};")
    header_lines.append(f"constexpr std::size_t kNumClasses = {len(label_classes)};")
    header_lines.append(f"constexpr std::size_t kNumLayers = {len(layers)};")
    header_lines.append("")
    header_lines.append("// Output interpretation:")
    header_lines.append("// - kBinaryLogit: final neuron logistic â†’ P(class[kLogitPositiveClass]), other class = 1 - P.")
    header_lines.append("// - kMultiClass: apply softmax to the final layer outputs to align with kClassNames order.")
    header_lines.append("enum class OutputType { kBinaryLogit, kMultiClass };")
    header_lines.append(f"constexpr OutputType kOutputType = OutputType::{output_type};")
    header_lines.append(
        f"constexpr std::size_t kLogitPositiveClass = {positive_class_index};  // valid only when kOutputType == OutputType::kBinaryLogit"
    )
    header_lines.append("")
    header_lines.append("static const char* const kClassNames[kNumClasses] = {")
    header_lines.append(format_array([f'"{c}"' for c in label_classes], indent="    ", per_line=4, formatter=str))
    header_lines.append("};")
    header_lines.append("")
    header_lines.append("static const char* const kFeatureNames[kInputDim] = {")
    header_lines.append(format_array([f'"{f}"' for f in feature_names], indent="    ", per_line=4, formatter=str))
    header_lines.append("};")
    header_lines.append("")
    header_lines.append("static const float kImputerMedian[kInputDim] = {")
    header_lines.append(format_array(imputer_stats))
    header_lines.append("};")
    header_lines.append("")
    header_lines.append("static const float kScalerMean[kInputDim] = {")
    header_lines.append(format_array(scaler_mean))
    header_lines.append("};")
    header_lines.append("")
    header_lines.append("static const float kScalerScale[kInputDim] = {")
    header_lines.append(format_array(scaler_scale))
    header_lines.append("};")
    header_lines.append("")
    header_lines.append("struct Layer {")
    header_lines.append("    std::size_t input_dim;")
    header_lines.append("    std::size_t output_dim;")
    header_lines.append("    const float* weights;  // row-major [output][input]")
    header_lines.append("    const float* biases;   // length == output_dim")
    header_lines.append("};")
    header_lines.append("")

    for layer in layers:
        idx = layer["index"]
        header_lines.append(f"static const float kLayer{idx}Weights[] = {{")
        header_lines.append(format_array(layer["weights"]))
        header_lines.append("};")
        header_lines.append(f"static const float kLayer{idx}Bias[] = {{")
        header_lines.append(format_array(layer["bias"]))
        header_lines.append("};")
        header_lines.append("")

    header_lines.append("static const Layer kLayers[kNumLayers] = {")
    layer_entries = []
    for layer in layers:
        idx = layer["index"]
        entry = (
            f"    {{ {layer['input_dim']}, {layer['output_dim']}, "
            f"kLayer{idx}Weights, kLayer{idx}Bias }}"
        )
        layer_entries.append(entry)
    header_lines.append(",\n".join(layer_entries))
    header_lines.append("};")
    header_lines.append("")
    header_lines.append("}  // namespace model_weights")
    header_lines.append("")
    return "\n".join(header_lines)


def export_header(model_dir: Path, out_path: Path) -> None:
    _pipeline, metadata, feature_names, label_classes, imputer, scaler, clf = load_artifacts(model_dir)

    imputer_stats = imputer.statistics_.tolist()
    scaler_mean = scaler.mean_.tolist()
    scaler_scale = scaler.scale_.tolist()

    if not (
        len(feature_names)
        == len(imputer_stats)
        == len(scaler_mean)
        == len(scaler_scale)
    ):
        raise SystemExit("Feature count mismatch between metadata and preprocessing steps.")

    layers = build_layer_params(clf)
    if not layers:
        raise SystemExit("No layers found in classifier.")

    output_neurons = layers[-1]["output_dim"]
    if len(label_classes) == 2 and output_neurons == 1:
        output_type = "kBinaryLogit"
        positive_class_index = 1
    elif output_neurons == len(label_classes):
        output_type = "kMultiClass"
        positive_class_index = 0
    else:
        raise SystemExit(
            "Unsupported classifier shape: final layer output does not align with number of classes. "
            "Ensure binary classifiers export a single logit and multi-class exports one logit per class."
        )

    header = generate_header(
        feature_names,
        label_classes,
        imputer_stats,
        scaler_mean,
        scaler_scale,
        layers,
        output_type,
        positive_class_index,
    )
    out_path.parent.mkdir(parents=True, exist_ok=True)
    out_path.write_text(header)

    print(f"Wrote header to {out_path}")
    print(f"Classes: {label_classes}")
    print(f"Feature order ({len(feature_names)}): {', '.join(feature_names)}")
    if output_type == "kBinaryLogit":
        print(f"Output mode: binary logit (probability corresponds to class '{label_classes[positive_class_index]}').")
    else:
        print("Output mode: multi-class softmax (probabilities follow kClassNames order).")
    if "val_accuracy" in metadata:
        print(f"Validation accuracy recorded during training: {metadata['val_accuracy']:.3f}")


def parse_args() -> argparse.Namespace:
    parser = argparse.ArgumentParser(description="Export trained MLP weights to a C header for Teensy inference.")
    parser.add_argument(
        "--modeldir",
        type=Path,
        default=Path("models") / "latest",
        help="Directory containing model.joblib and training_metadata.json (default: models/latest).",
    )
    parser.add_argument(
        "--out",
        type=Path,
        default=Path("include") / "model_weights.h",
        help="Path to write the generated header (default: include/model_weights.h).",
    )
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    export_header(args.modeldir, args.out)
